{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ea76e2d8",
      "metadata": {
        "id": "ea76e2d8"
      },
      "source": [
        "# Интуиция CLIP\n",
        "\n",
        "В этом блоке мы **симулируем CLIP**. В качестве датасета возьмем объекты, у которых есть по две \"модальности\", каждая из которых представлена двумерным эмбеддингом. Эквивалентами таких эмбеддингов могут быть выходы маломерных энкодеров различных модальностей А и B, например: A — «картинка», B — «текст».\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81e18664",
      "metadata": {
        "id": "81e18664"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Cfg:\n",
        "    n_pairs: int = 256     # число пар (A,B)\n",
        "    d: int = 2             # размерность эмбеддингов (2D для красивой визуализации)\n",
        "    tau: float = 0.07      # температура (как в CLIP: logits / tau)\n",
        "    epochs: int = 100\n",
        "    vis_every: int = 20\n",
        "    lr: float = 2e-2\n",
        "    wd: float = 1e-4\n",
        "\n",
        "cfg = Cfg()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6a935c",
      "metadata": {
        "id": "ea6a935c"
      },
      "source": [
        "### Обучаемые эмбеддинги вместо энкодеров\n",
        "\n",
        "Вместо настоящих энкодеров заведём **два набора параметров** `A` и `B` (по вектору на каждую пару).  \n",
        "Будем обучать их так, чтобы эмбеддинги внутри пары сближались, а вне — отталкивались.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24317d2a",
      "metadata": {
        "id": "24317d2a"
      },
      "outputs": [],
      "source": [
        "# обучаемые параметры — \"выходы\" визуального и текстового энкодеров\n",
        "A = torch.nn.Parameter(torch.randn(cfg.n_pairs, cfg.d, device=device))\n",
        "B = torch.nn.Parameter(torch.randn(cfg.n_pairs, cfg.d, device=device))\n",
        "\n",
        "opt = torch.optim.AdamW([A, B], lr=cfg.lr, weight_decay=cfg.wd)\n",
        "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "print(\"A:\", tuple(A.shape), \"B:\", tuple(B.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec2a688",
      "metadata": {
        "id": "1ec2a688"
      },
      "source": [
        "### Контрастивные лоссы\n",
        "\n",
        "**CLIP-стиль (InfoNCE / softmax):**\n",
        "- Нормализуем векторы.\n",
        "- Строим логиты как косинусные похожести / `tau`.\n",
        "- Кросс-энтропия для обоих направлений (A→B и B→A) и усредняем.\n",
        "\n",
        "**Sigmoid-стиль (в духе SigLIP):**\n",
        "- Рассматриваем **все пары** независимо.\n",
        "- Подаём все косинусные похожести в BCE-with-logits, где таргет — единицы на диагонали (позитивы) и нули вне диагонали (негативы)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fa8ece",
      "metadata": {
        "id": "16fa8ece"
      },
      "outputs": [],
      "source": [
        "def clip_bidir_loss(a, b, tau: float):\n",
        "    a = F.normalize(a, dim=-1)\n",
        "    b = F.normalize(b, dim=-1)\n",
        "    logits_ab = (a @ b.T) / tau\n",
        "    logits_ba = (b @ a.T) / tau\n",
        "    target = torch.arange(a.size(0), device=a.device)\n",
        "    loss_ab = F.cross_entropy(logits_ab, target)\n",
        "    loss_ba = F.cross_entropy(logits_ba, target)\n",
        "    return 0.5 * (loss_ab + loss_ba), logits_ab\n",
        "\n",
        "def sigmoid_pairwise_loss(a, b, scale: float = 10.0):\n",
        "    a = F.normalize(a, dim=-1)\n",
        "    b = F.normalize(b, dim=-1)\n",
        "    sims = (a @ b.T) * scale\n",
        "    targets = torch.eye(a.size(0), device=a.device)\n",
        "    loss = F.binary_cross_entropy_with_logits(sims, targets)\n",
        "    return loss, sims"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776b9be8",
      "metadata": {
        "id": "776b9be8"
      },
      "source": [
        "Реализуем функцию, которая будет строить необходимые визуализации в процессе обучения.\n",
        "\n",
        "Визуализирует эмбеддинги A и B с соединениями и статистикой.\n",
        "\n",
        "    - Нормализуем для отображения\n",
        "\n",
        "    - Отдельные scatter для A и для B (разные маркеры)\n",
        "\n",
        "    - Совмещённый график с линиями между соответствующими парами\n",
        "    \n",
        "    - Распределение косинусных расстояний (1 - cos) для позитивных пар\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ae2c3f",
      "metadata": {
        "id": "e6ae2c3f"
      },
      "outputs": [],
      "source": [
        "def visualize(a, b, epoch, loss_value, logits_ab=None, max_show=20, show_all=False):\n",
        "\n",
        "    a_n = F.normalize(a, dim=-1).detach().cpu().numpy()\n",
        "    b_n = F.normalize(b, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "    n_total = a_n.shape[0]\n",
        "    n_show = n_total if show_all else min(max_show, n_total)\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, n_show))\n",
        "\n",
        "    # Вывод Top-1 A→B при наличии logits_ab\n",
        "    if logits_ab is not None:\n",
        "        with torch.no_grad():\n",
        "            preds = logits_ab.argmax(dim=1)\n",
        "            target = torch.arange(logits_ab.size(0), device=logits_ab.device)\n",
        "            acc = (preds == target).float().mean().item()\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    ax1, ax2, ax3, ax4 = axes.flat\n",
        "\n",
        "    for i in range(n_show):\n",
        "        ax1.scatter(a_n[i, 0], a_n[i, 1],\n",
        "                    c=[colors[i]], s=100, marker=\"o\",\n",
        "                    edgecolors=\"black\", linewidth=1, alpha=0.8)\n",
        "        if n_show <= 20:\n",
        "            ax1.text(a_n[i, 0], a_n[i, 1] + 0.05, f\"A{i+1}\",\n",
        "                     ha=\"center\", fontsize=8, fontweight=\"bold\")\n",
        "    ax1.add_patch(plt.Circle((0, 0), 1, fill=False, ls=\"--\", alpha=0.5, color=\"gray\"))\n",
        "    ax1.set_title(f\"A\")\n",
        "    ax1.set_xlim(-1.1, 1.1); ax1.set_ylim(-1.1, 1.1)\n",
        "    ax1.set_aspect(\"equal\"); ax1.grid(alpha=0.3)\n",
        "\n",
        "    for i in range(n_show):\n",
        "        ax2.scatter(b_n[i, 0], b_n[i, 1],\n",
        "                    c=[colors[i]], s=100, marker=\"s\",\n",
        "                    edgecolors=\"black\", linewidth=1, alpha=0.8)\n",
        "        if n_show <= 20:\n",
        "            ax2.text(b_n[i, 0], b_n[i, 1] + 0.05, f\"B{i+1}\",\n",
        "                     ha=\"center\", fontsize=8, fontweight=\"bold\")\n",
        "    ax2.add_patch(plt.Circle((0, 0), 1, fill=False, ls=\"--\", alpha=0.5, color=\"gray\"))\n",
        "    ax2.set_title(f\"B\")\n",
        "    ax2.set_xlim(-1.1, 1.1); ax2.set_ylim(-1.1, 1.1)\n",
        "    ax2.set_aspect(\"equal\"); ax2.grid(alpha=0.3)\n",
        "\n",
        "    for i in range(n_show):\n",
        "        ax3.scatter(a_n[i, 0], a_n[i, 1],\n",
        "                    c=[colors[i]], s=80, marker=\"o\",\n",
        "                    edgecolors=\"black\", linewidth=1, alpha=0.85, label=\"A\" if i==0 else \"\")\n",
        "        ax3.scatter(b_n[i, 0], b_n[i, 1],\n",
        "                    c=[colors[i]], s=80, marker=\"s\",\n",
        "                    edgecolors=\"black\", linewidth=1, alpha=0.85, label=\"B\" if i==0 else \"\")\n",
        "        ax3.plot([a_n[i, 0], b_n[i, 0]],\n",
        "                 [a_n[i, 1], b_n[i, 1]],\n",
        "                 \"k--\", alpha=0.35, linewidth=0.7)\n",
        "    ax3.add_patch(plt.Circle((0, 0), 1, fill=False, ls=\"--\", alpha=0.5, color=\"gray\"))\n",
        "    ax3.set_title(f\"Совмещённые пары\\nTop-1 A→B accuracy: {acc*100:.2f}%\")\n",
        "    ax3.set_xlim(-1.1, 1.1); ax3.set_ylim(-1.1, 1.1)\n",
        "    ax3.set_aspect(\"equal\"); ax3.grid(alpha=0.3)\n",
        "    ax3.legend(loc=\"lower left\")\n",
        "\n",
        "    # --- (1,1) распределение косинусных расстояний 1 - cos(A_i, B_i) ---\n",
        "    sims = (F.normalize(a, dim=-1) @ F.normalize(b, dim=-1).T).detach().cpu().numpy()\n",
        "    diag_sims = np.diag(sims)\n",
        "    cos_dists = 1.0 - diag_sims\n",
        "    ax4.hist(cos_dists, bins=20, alpha=0.8, color=\"skyblue\", edgecolor=\"black\")\n",
        "    mean_dist = float(np.mean(cos_dists))\n",
        "    std_dist = float(np.std(cos_dists))\n",
        "    ax4.axvline(mean_dist, color=\"red\", ls=\"--\", lw=2, label=f\"среднее={mean_dist:.3f}\")\n",
        "    ax4.set_title(\"Распределение косинусных расстояний\")\n",
        "    ax4.set_xlabel(\"1 - cos(A_i, B_i)\")\n",
        "    ax4.set_ylabel(\"Частота\")\n",
        "    ax4.grid(alpha=0.3)\n",
        "    ax4.legend()\n",
        "\n",
        "    fig.suptitle(f\"Эпоха {epoch} | loss={loss_value:.4f}\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa002de9",
      "metadata": {
        "id": "fa002de9"
      },
      "source": [
        "Обучаем `A` и `B` так, чтобы A↔B совпадали. Каждые несколько эпох смотрим на геометрию и метрики.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7a1b85",
      "metadata": {
        "id": "7c7a1b85"
      },
      "outputs": [],
      "source": [
        "loss_history = []\n",
        "\n",
        "for epoch in range(1, cfg.epochs + 1):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    loss, logits_ab = clip_bidir_loss(A, B, cfg.tau)\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_([A, B], max_norm=1.0)\n",
        "    opt.step()\n",
        "    sched.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    if epoch % cfg.vis_every == 0 or epoch in (1, cfg.epochs):\n",
        "        visualize(A, B, epoch, loss.item(), logits_ab=logits_ab)\n",
        "\n",
        "# В конце обучения рисуем график потерь\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(loss_history)\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"История обучения\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ce734c",
      "metadata": {
        "id": "18ce734c"
      },
      "source": [
        "# Zero-shot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4586cf",
      "metadata": {
        "id": "5c4586cf"
      },
      "source": [
        "Обычно для классификации нужно обучать модель на размеченном датасете. Но CLIP позволяет делать **zero-shot**:  \n",
        "- Мы задаём список текстовых описаний (например, \"футболка\", \"ноутбук\", \"чашка\").  \n",
        "- Модель превращает текст и изображения в одно пространство эмбеддингов.  \n",
        "- Класс для картинки выбирается как тот текст, чей эмбеддинг ближе всего к эмбеддингу изображения.  \n",
        "\n",
        "Этот подход позволяет модели без дополнительного обучения решать задачи классификации и поиска.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2251d4",
      "metadata": {
        "id": "8f2251d4"
      },
      "outputs": [],
      "source": [
        "# Установка и загрузка зависимостей (если в colab/kaggle)\n",
        "!pip install datasets transformers accelerate faiss-cpu umap-learn plotly seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27027747",
      "metadata": {
        "id": "27027747"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import random\n",
        "import re\n",
        "import collections\n",
        "import pathlib\n",
        "from math import ceil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import umap\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374aa3d7",
      "metadata": {
        "id": "374aa3d7"
      },
      "source": [
        "- **Датасет**: используем подмножество Amazon Products (5k примеров), так как это реальные изображения товаров с категориями. Такой датасет позволяет тестировать zero-shot классификацию и retrieval.  \n",
        "- **Модель CLIP**: берём `openai/clip-vit-base-patch32`.  \n",
        "  - ViT-B/32 — базовая версия, работает быстрее, чем более тяжёлые варианты.  \n",
        "  - Для серьёзных задач можно заменить на SigLIP или большие версии CLIP.  \n",
        "- **CLIPProcessor**: включает в себя токенизацию текста и препроцессинг изображений (resize, crop, normalization).  \n",
        "- **Устройство**: проверяем доступность CUDA (NVIDIA GPU) или MPS (Apple Silicon).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa87da5",
      "metadata": {
        "id": "2fa87da5"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else (\n",
        "    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "print(\"device:\", device)\n",
        "\n",
        "dataset = load_dataset(\"milistu/AMAZON-Products-2023\", split=\"train[:5000]\")\n",
        "print(\"dataset:\", dataset)\n",
        "\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(MODEL_NAME).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "print(\"CLIP загружен:\", MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a50314f",
      "metadata": {
        "id": "7a50314f"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# Размер подмножества для демо (адаптируется к среде)\n",
        "BATCH_SIZE = 8\n",
        "N_SAMPLES = int(min(2000, len(dataset)))\n",
        "BATCH_EMB = max(4, BATCH_SIZE * (2 if device == 'cpu' else 4))\n",
        "\n",
        "print(f\"SEED={SEED} | N_SAMPLES={N_SAMPLES} | BATCH_EMB={BATCH_EMB} | device={device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e35ce4c",
      "metadata": {
        "id": "9e35ce4c"
      },
      "source": [
        "`to_pil`: универсальная функция для приведения изображения к формату `PIL.Image`. Поддерживает строки (пути, URL), numpy-массивы и байты.  \n",
        "`display_images`: удобная функция для вывода сразу нескольких изображений в ряд. Это поможет наглядно показывать примеры при zero-shot классификации и retrieval.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beea0402",
      "metadata": {
        "id": "beea0402"
      },
      "outputs": [],
      "source": [
        "def to_pil(img):\n",
        "    \"Универсально приводим к PIL.Image RGB.\"\n",
        "    if isinstance(img, Image.Image):\n",
        "        return img.convert(\"RGB\")\n",
        "    if isinstance(img, bytes):\n",
        "        return Image.open(io.BytesIO(img)).convert(\"RGB\")\n",
        "    if isinstance(img, str):\n",
        "        if img.startswith(\"http://\") or img.startswith(\"https://\"):\n",
        "            r = requests.get(img, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
        "        return Image.open(img).convert(\"RGB\")\n",
        "    if isinstance(img, np.ndarray):\n",
        "        if img.ndim == 2:  # grayscale\n",
        "            return Image.fromarray(img).convert(\"RGB\")\n",
        "        return Image.fromarray(img[..., :3])\n",
        "    return img  # надеемся, что это уже PIL\n",
        "\n",
        "def display_images(images, titles=None, figsize=(16, 4)):\n",
        "    \"Отрисовка ряда изображений.\"\n",
        "    n = len(images)\n",
        "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
        "    if n == 1: axes = [axes]\n",
        "    for i, im in enumerate(images):\n",
        "        axes[i].imshow(to_pil(im))\n",
        "        axes[i].axis(\"off\")\n",
        "        if titles and i < len(titles):\n",
        "            axes[i].set_title(titles[i], fontsize=10)\n",
        "    plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82a3b01d",
      "metadata": {
        "id": "82a3b01d"
      },
      "source": [
        "Каждое изображение и текст CLIP превращает в вектор фиксированной длины (512 в случае нашей модели).  \n",
        "Эти векторы лежат в общем пространстве, где можно сравнивать текст и картинки напрямую.\n",
        "\n",
        "Чтобы не пересчитывать эмбеддинги заново каждый раз:  \n",
        "- Считаем их батчами (для ускорения на GPU).  \n",
        "- Кладём в кэш (например, `.npy` или `.pt` файлы).  \n",
        "\n",
        "Многие промышленные системы (поиск, рекомендация, deduplication) сначала считают эмбеддинги, а потом работают только с ними в различных дополнительных индексах и сценариях.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d123b042",
      "metadata": {
        "id": "d123b042"
      },
      "outputs": [],
      "source": [
        "CACHE_DIR = pathlib.Path(\"./_cache\"); CACHE_DIR.mkdir(exist_ok=True)\n",
        "EMB_CACHE = CACHE_DIR / f\"clip_img_emb_vitb32_N{N_SAMPLES}.npy\"\n",
        "\n",
        "def embed_images_clip(images, model, processor, device, batch_size=32):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    feats = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(images), batch_size), desc=\"CLIP image features\"):\n",
        "            batch = [to_pil(im) for im in images[i:i+batch_size]]\n",
        "            inputs = processor(images=batch, return_tensors=\"pt\", padding=True).to(device)\n",
        "            out = model.get_image_features(**inputs)\n",
        "            out = F.normalize(out, p=2, dim=1)  # косинусная геометрия\n",
        "            feats.append(out.cpu())\n",
        "    return torch.cat(feats, dim=0).numpy().astype(\"float32\")\n",
        "\n",
        "# Собираем первые N_SAMPLES изображений (PIL)\n",
        "images_list = [dataset[i][\"image\"] for i in range(N_SAMPLES)]\n",
        "\n",
        "# Загружаем из кэша или считаем\n",
        "if EMB_CACHE.exists():\n",
        "    img_feats = np.load(EMB_CACHE)\n",
        "    if img_feats.shape[0] != N_SAMPLES:\n",
        "        img_feats = embed_images_clip(images_list, clip_model, clip_processor, device, BATCH_EMB)\n",
        "        np.save(EMB_CACHE, img_feats)\n",
        "else:\n",
        "    img_feats = embed_images_clip(images_list, clip_model, clip_processor, device, BATCH_EMB)\n",
        "    np.save(EMB_CACHE, img_feats)\n",
        "\n",
        "assert img_feats.shape[0] == N_SAMPLES\n",
        "print(\"Эмбеддинги готовы:\", img_feats.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f3dbd5",
      "metadata": {
        "id": "86f3dbd5"
      },
      "source": [
        "\n",
        "1) Автоматически соберём **топ-категории** из датасета (поля `main_category|category|categories`), c нормализацией текста.  \n",
        "2) Функция `clip_zeroshot_probs(image, labels)` вернёт распределение по лейблам.  \n",
        "3) Сконструируем **heatmap**: несколько картинок × несколько категорий.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31ba1c6",
      "metadata": {
        "id": "f31ba1c6"
      },
      "outputs": [],
      "source": [
        "def normalize_label(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"[_/|]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def collect_top_categories(ds, limit=2000, topk=10):\n",
        "    counts = collections.Counter()\n",
        "    n = min(len(ds), limit)\n",
        "    for i in range(n):\n",
        "        row = ds[i]\n",
        "        cat = None\n",
        "        for key in (\"main_category\", \"category\", \"categories\"):\n",
        "            if key in row and row[key]:\n",
        "                cat = row[key]; break\n",
        "        if isinstance(cat, list) and cat:\n",
        "            cat = cat[0]\n",
        "        if isinstance(cat, str):\n",
        "            counts[normalize_label(cat)] += 1\n",
        "    top = [c for c,_ in counts.most_common(topk)]\n",
        "    if not top:\n",
        "        top = [\"electronics\",\"clothing\",\"books\",\"home and garden\",\"sports and outdoors\",\n",
        "               \"beauty and personal care\",\"toys and games\",\"automotive\",\"health and household\",\"food and beverages\"]\n",
        "    return top, counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8127c1",
      "metadata": {
        "id": "bf8127c1"
      },
      "outputs": [],
      "source": [
        "product_labels, cat_counts = collect_top_categories(dataset, limit=3000, topk=8)\n",
        "print(\"🎯 Категории для zero-shot:\", product_labels)\n",
        "\n",
        "def clip_zeroshot_probs(image, text_labels, model, processor, device):\n",
        "    pil = to_pil(image)\n",
        "    inputs = processor(text=text_labels, images=pil, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs)\n",
        "        probs = out.logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n",
        "    return dict(zip(text_labels, probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23523ed2",
      "metadata": {
        "id": "23523ed2"
      },
      "outputs": [],
      "source": [
        "# sanity-check\n",
        "probs0 = clip_zeroshot_probs(images_list[0], product_labels, clip_model, clip_processor, device)\n",
        "list(probs0.items())[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a2fb17",
      "metadata": {
        "id": "16a2fb17"
      },
      "outputs": [],
      "source": [
        "# Выбираем несколько индексов равномерно по коллекции\n",
        "idxs = np.linspace(0, N_SAMPLES-1, num=min(6, N_SAMPLES), dtype=int)\n",
        "heat = []\n",
        "for idx in idxs:\n",
        "    pr = clip_zeroshot_probs(images_list[idx], product_labels, clip_model, clip_processor, device)\n",
        "    heat.append([pr[l] for l in product_labels])\n",
        "\n",
        "df_heat = pd.DataFrame(heat, index=[f\"img {i}\" for i in idxs], columns=product_labels)\n",
        "\n",
        "display_images([images_list[i] for i in idxs],\n",
        "               [f\"img {i}\" for i in idxs],\n",
        "               figsize=(2.8*len(idxs), 3.2))\n",
        "\n",
        "plt.figure(figsize=(1.2*len(product_labels)+4, 0.7*len(idxs)+4))\n",
        "sns.heatmap(df_heat, annot=True, fmt=\".2f\", cmap=\"YlOrRd\", cbar_kws={'shrink':0.8})\n",
        "plt.title(\"Zero-shot: распределение вероятностей по категориям\")\n",
        "plt.xlabel(\"Категории\"); plt.ylabel(\"Изображения\")\n",
        "plt.xticks(rotation=35, ha=\"right\"); plt.yticks(rotation=0)\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e263b8",
      "metadata": {
        "id": "f0e263b8"
      },
      "source": [
        "# Faiss индекс"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6669cc",
      "metadata": {
        "id": "6b6669cc"
      },
      "source": [
        "Когда мы имеем тысячи или миллионы изображений, простой перебор (сравнение каждого эмбеддинга с каждым запросом) становится слишком дорогим:  \n",
        "- При 1 млн объектов нужно сделать 1 млн × 512 операций для каждого запроса.  \n",
        "- Для 100 млн объектов это уже неподъёмно.  \n",
        "\n",
        "💡 Решение: использовать **Approximate Nearest Neighbors (ANN)** — алгоритмы приближённого поиска ближайших соседей.  \n",
        "\n",
        "**FAISS (Facebook AI Similarity Search)** — это библиотека, разработанная в Meta AI, которая:  \n",
        "- Позволяет строить индексы для эмбеддингов.  \n",
        "- Ускоряет поиск в сотни раз.  \n",
        "- Поддерживает как точные (`IndexFlatL2`), так и приближённые (`IVF`, `PQ`, `HNSW`) индексы.  \n",
        "\n",
        "**Примеры индексов:**\n",
        "- `IndexFlatL2`: хранит все эмбеддинги, поиск точный, но медленный (O(N)). Хорош для экспериментов и маленьких датасетов.  \n",
        "- `IndexIVF`: использует «инвертированный файл»: сначала ищем ближайший кластер, затем ближайших соседей внутри него. Значительно быстрее.  \n",
        "- `IndexPQ`: хранит сжатые вектора, экономит память (важно для 100+ млн объектов).  \n",
        "- `HNSW`: графовая структура поиска, баланс скорости и качества.  \n",
        "\n",
        "**Почему это важно:**  \n",
        "- Именно так работают современные поисковые и рекомендательные системы.  \n",
        "- CLIP-эмбеддинги → FAISS / HNSW индексы → быстрый поиск картинок/видео/товаров по тексту или любым другим векторным признакам.  \n",
        "\n",
        "В нашем примере мы построим простой `IndexFlatL2`, чтобы понять механику, а затем можно заменить его на более сложный (IVF/PQ) для больших датасетов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63de1595",
      "metadata": {
        "id": "63de1595"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Индекс на косинусных (через inner product)\n",
        "d = img_feats.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(img_feats.astype(\"float32\"))\n",
        "print(f\"✅ FAISS: d={d}, ntotal={index.ntotal}\")\n",
        "\n",
        "def search_by_image_idx(query_idx, k=6):\n",
        "    q = img_feats[query_idx].reshape(1,-1).astype(\"float32\")\n",
        "    D, I = index.search(q, k)\n",
        "    return I[0], D[0]\n",
        "\n",
        "def encode_text_norm(text: str) -> np.ndarray:\n",
        "    inp = clip_processor(text=[text], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        t = clip_model.get_text_features(**inp)\n",
        "        t = F.normalize(t, p=2, dim=1)\n",
        "    return t.cpu().numpy().astype(\"float32\")[0]\n",
        "\n",
        "def search_by_text(query: str, k=6):\n",
        "    q = encode_text_norm(query).reshape(1,-1)\n",
        "    D, I = index.search(q, k)\n",
        "    return I[0], D[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ca6312",
      "metadata": {
        "id": "d9ca6312"
      },
      "outputs": [],
      "source": [
        "# image→image\n",
        "q_idx = int(N_SAMPLES * 0.37)\n",
        "nbrs, sims = search_by_image_idx(q_idx, k=6)\n",
        "print(\"🔍 image→image:\", list(zip(nbrs, sims)))\n",
        "display_images([images_list[q_idx]] + [images_list[i] for i in nbrs],\n",
        "               [\"QUERY\"] + [f\"sim={s:.2f}\" for s in sims], figsize=(18,3))\n",
        "\n",
        "# text→image\n",
        "q_text = \"traditional percussion instrument\"\n",
        "nbrs_t, sims_t = search_by_text(q_text, k=6)\n",
        "print(\"🔎 text→image:\", q_text, \"→\", list(zip(nbrs_t, sims_t)))\n",
        "display_images([images_list[i] for i in nbrs_t],\n",
        "               [f\"{i}  sim={s:.2f}\" for i,s in zip(nbrs_t, sims_t)], figsize=(18,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91af7a12",
      "metadata": {
        "id": "91af7a12"
      },
      "source": [
        "Эмбеддинги в CLIP позволяют производить с ними векторную арифметику, поэтому можно складывать и вычитать смыслы.  \n",
        "Естественно, это не всегда работает идеально, но даёт интуицию: **пространство эмбеддингов хранит семантику**. Благодаря этому мы можем находить направления, отвечающие за те или иные свойства, что может помочь в дальнейших экспериментах (за рамками данного ноутбука) для контролируемой генерации.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89860663",
      "metadata": {
        "id": "89860663"
      },
      "outputs": [],
      "source": [
        "def vector_arithmetic_search(base: str, minus: str, plus: str, k: int = 6):\n",
        "    b = encode_text_norm(base)\n",
        "    m = encode_text_norm(minus)\n",
        "    p = encode_text_norm(plus)\n",
        "    q = b - m + p\n",
        "    q = q / (np.linalg.norm(q) + 1e-9)\n",
        "    D, I = index.search(q.reshape(1,-1).astype(\"float32\"), k)\n",
        "    display_images([images_list[i] for i in I[0]],\n",
        "                   [f\"{i}  sim={s:.2f}\" for i,s in zip(I[0], D[0])],\n",
        "                   figsize=(18,3))\n",
        "    print(f\"🧮 '{base}' - '{minus}' + '{plus}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d752529a",
      "metadata": {
        "id": "d752529a"
      },
      "outputs": [],
      "source": [
        "vector_arithmetic_search(\"electric guitar\", \"electric\", \"acoustic\", k=6)\n",
        "vector_arithmetic_search(\"blue backpack\", \"blue\", \"red\", k=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa042816",
      "metadata": {
        "id": "fa042816"
      },
      "source": [
        "# Визуализация пространства эмбеддингов\n",
        "\n",
        "- У CLIP эмбеддинги размерности 512. Визуализировать напрямую невозможно.  \n",
        "- Используем **UMAP** — метод снижения размерности, который сохраняет локальные структуры (похож на t-SNE, но быстрее и воспроизводим, так как имеет метод `transform`).  \n",
        "- Получаем 2D-карту, где похожие объекты лежат рядом.  \n",
        "\n",
        "Когда мы делаем кластеризацию (например, KMeans), возникает вопрос: **сколько кластеров выбрать?**\n",
        "\n",
        "Для этого используют метрику **Silhouette score**:\n",
        "\n",
        "- Для каждой точки измеряется:\n",
        "  - *a* — насколько точка близка к точкам своего кластера.\n",
        "  - *b* — насколько она далека от точек ближайшего чужого кластера.\n",
        "- Score = (b - a) / max(a, b).\n",
        "\n",
        "Интерпретация:\n",
        "- Ближе к **1** → хороший кластер (точки плотные и далеко от других кластеров).\n",
        "- Около **0** → точка на границе кластеров.\n",
        "- Отрицательное значение → точка «ошиблась кластером».\n",
        "\n",
        "Средний silhouette score помогает подобрать оптимальное число кластеров **K**: там, где метрика максимальна.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311ac57b",
      "metadata": {
        "id": "311ac57b"
      },
      "outputs": [],
      "source": [
        "# UMAP\n",
        "reducer = umap.UMAP(n_components=2, n_neighbors=20, min_dist=0.5, metric=\"cosine\", random_state=SEED)\n",
        "emb2d = reducer.fit_transform(img_feats)\n",
        "\n",
        "# Автоподбор K\n",
        "K_CAND = [5, 8, 10, 12]\n",
        "best_k, best_score = None, -1\n",
        "for k in K_CAND:\n",
        "    km = KMeans(n_clusters=k, n_init=10, random_state=SEED)\n",
        "    labels = km.fit_predict(emb2d)\n",
        "    score = silhouette_score(emb2d, labels)\n",
        "    if score > best_score:\n",
        "        best_k, best_score = k, score\n",
        "print(f\"Лучший K={best_k} (silhouette={best_score:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db56c95",
      "metadata": {
        "id": "9db56c95"
      },
      "outputs": [],
      "source": [
        "km = KMeans(n_clusters=best_k, n_init=20, random_state=SEED).fit(emb2d)\n",
        "clusters = km.labels_\n",
        "\n",
        "df2d = pd.DataFrame({\n",
        "    \"x\": emb2d[:,0], \"y\": emb2d[:,1],\n",
        "    \"cluster\": clusters,\n",
        "    \"title\": [dataset[i].get(\"title\",\"\") for i in range(N_SAMPLES)],\n",
        "    \"idx\": np.arange(N_SAMPLES)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f411668",
      "metadata": {
        "id": "5f411668"
      },
      "outputs": [],
      "source": [
        "# Статический scatter\n",
        "plt.figure(figsize=(8,6))\n",
        "for c in range(best_k):\n",
        "    m = clusters==c\n",
        "    plt.scatter(df2d.loc[m,\"x\"], df2d.loc[m,\"y\"], s=12, alpha=0.7, label=f\"C{c}\")\n",
        "plt.legend(title=\"cluster\", ncol=best_k//5+1)\n",
        "plt.title(\"UMAP 2D (CLIP image embeddings)\")\n",
        "plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "# Интерактивный Plotly\n",
        "fig = px.scatter(df2d, x=\"x\", y=\"y\", color=df2d[\"cluster\"].astype(str),\n",
        "                 hover_data={\"idx\":True, \"title\":True, \"cluster\":True},\n",
        "                 title=\"UMAP 2D — интерактивно (наведите курсор)\")\n",
        "fig.update_layout(width=800, height=600, legend_title_text=\"cluster\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7309d515",
      "metadata": {
        "id": "7309d515"
      },
      "source": [
        "# Интерактивное демо\n",
        "\n",
        "Здесь мы объединяем всё:  \n",
        "- Выбираем картинку.  \n",
        "- Смотрим top-5 категорий (zero-shot).  \n",
        "- Находим похожие изображения в FAISS.  \n",
        "- Смотрим на её позицию на UMAP-карте.  \n",
        "\n",
        "Такой интерактив помогает:  \n",
        "- лучше понять, как работает модель,  \n",
        "- почувствовать ограничения (ошибки модели, шумные данные),  \n",
        "- увидеть практическую ценность мультимодальных эмбеддингов.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d623b979",
      "metadata": {
        "id": "d623b979"
      },
      "outputs": [],
      "source": [
        "def clip_topk(image, labels, k=5):\n",
        "    pr = clip_zeroshot_probs(image, labels, clip_model, clip_processor, device)\n",
        "    return sorted(pr.items(), key=lambda x: x[1], reverse=True)[:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dc3506",
      "metadata": {
        "id": "07dc3506"
      },
      "outputs": [],
      "source": [
        "def interactive_demo(i: int, k_neighbors: int = 8, labels=None, cols: int = 2):\n",
        "    labels = labels or product_labels\n",
        "    assert 0 <= i < N_SAMPLES\n",
        "\n",
        "    top5 = clip_topk(images_list[i], labels, k=5)\n",
        "    nbrs, sims = search_by_image_idx(i, k=k_neighbors)\n",
        "\n",
        "    x, y = df2d.loc[i, \"x\"], df2d.loc[i, \"y\"]\n",
        "    cl = int(df2d.loc[i, \"cluster\"])\n",
        "\n",
        "    # размеры фигуры: правая часть растягивается под число рядов\n",
        "    rows = ceil(len(nbrs) / cols)\n",
        "    H = max(7, 2 + 2.2 * rows)   # высота\n",
        "    W = 16                       # ширина\n",
        "    fig = plt.figure(figsize=(W, H))\n",
        "    gs = GridSpec(2, 3, figure=fig,\n",
        "                  width_ratios=[1.2, 1.0, 1.4],\n",
        "                  height_ratios=[1, 1],\n",
        "                  wspace=0.35, hspace=0.28)\n",
        "\n",
        "    # левый столбец — исходник\n",
        "    ax_img = fig.add_subplot(gs[:, 0])\n",
        "    ax_img.imshow(to_pil(images_list[i])); ax_img.axis(\"off\")\n",
        "    ax_img.set_title(f\"img {i} | cluster={cl}\")\n",
        "\n",
        "    # верх-центр — топ-5 классов\n",
        "    ax_bar = fig.add_subplot(gs[0, 1])\n",
        "    labs, probs = zip(*top5)\n",
        "    ax_bar.barh(range(len(labs)), probs)\n",
        "    ax_bar.set_yticks(range(len(labs))); ax_bar.set_yticklabels(labs)\n",
        "    ax_bar.invert_yaxis(); ax_bar.set_xlim(0, 1)\n",
        "    ax_bar.set_title(\"Top-5 zero-shot probs\")\n",
        "\n",
        "    # низ-центр — UMAP позиция\n",
        "    ax_umap = fig.add_subplot(gs[1, 1])\n",
        "    ax_umap.scatter(df2d[\"x\"], df2d[\"y\"], s=6, alpha=0.15, color=\"gray\")\n",
        "    ax_umap.scatter([x], [y], s=90, edgecolor=\"k\")\n",
        "    ax_umap.set_title(\"UMAP position\"); ax_umap.grid(alpha=0.3)\n",
        "\n",
        "    # правый столбец — грид из соседей (rows x cols)\n",
        "    right = gs[:, 2].subgridspec(rows, cols, wspace=0.08, hspace=0.12)\n",
        "    for idx, (j, s) in enumerate(zip(nbrs, sims)):\n",
        "        r, c = divmod(idx, cols)\n",
        "        ax = fig.add_subplot(right[r, c])\n",
        "        ax.imshow(to_pil(images_list[j])); ax.axis(\"off\")\n",
        "        ax.set_title(f\"{j}  sim={s:.2f}\", fontsize=9, pad=2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f1e354",
      "metadata": {
        "id": "38f1e354"
      },
      "outputs": [],
      "source": [
        "interactive_demo(0, k_neighbors=8, cols=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443c0b47",
      "metadata": {
        "id": "443c0b47"
      },
      "outputs": [],
      "source": [
        "interactive_demo(1000, k_neighbors=8, cols=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffe1000",
      "metadata": {
        "id": "8ffe1000"
      },
      "outputs": [],
      "source": [
        "interactive_demo(1500, k_neighbors=8, cols=2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}